{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e979f708053d08a14cb92e0af625f5b6ed44481"
   },
   "source": [
    "# NDSC Beginner (Text Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f68c8f72e568888a59babb997449b92878286f26"
   },
   "source": [
    "### File preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "af9643775abf006b7be0916280a55f8e83e6b06d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math as math\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Read from the file\n",
    "df = pd.read_csv(\"../input/train.csv\")\n",
    "\n",
    "# sample only the first few records\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "df_train = df[:]\n",
    "\n",
    "# get the big label from the image path\n",
    "output = []\n",
    "\n",
    "for label in enumerate(df_train['image_path']):\n",
    "    output.append(label[1].split('/')[0].split('_')[0].capitalize())\n",
    "df_train['Category_Type'] = output\n",
    "    \n",
    "# print the first view for vieweing\n",
    "df_train.head()\n",
    "df_train.info()\n",
    "\n",
    "sys.stdout.write('File preparation done...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "35aaba85edd04506912c405e5e63488b790e3d84"
   },
   "source": [
    "### Convert json file into dictonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e849ed45adf43812977ddb23ec7c3e7d831354b0"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "with open('input/categories.json') as f:\n",
    "    categories_json = json.load(f)\n",
    "    \n",
    "id_to_category = {}\n",
    "category_to_id = {}\n",
    "\n",
    "for category_class in categories_json:\n",
    "    for category_name in categories_json[category_class]:\n",
    "        id_to_category[categories_json[category_class][category_name]] = category_name\n",
    "        category_to_id[category_name] = categories_json[category_class][category_name]\n",
    "        \n",
    "pprint.pprint(id_to_category)\n",
    "sys.stdout.write('Conversion of JSON done...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ae20b769d39ea378a506c4435d6642e3aa1bcdb8"
   },
   "source": [
    "### Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "635ae9670c1f91b72a42e479a45994aed6e68021",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# remove single charcter\n",
    "def keepAlpha(sentence):\n",
    "    alpha_sent = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence)\n",
    "    return alpha_sent\n",
    "\n",
    "# remove digit\n",
    "def removeDigit(sentence):\n",
    "    result_sent = re.sub(r'\\d', 'X', sentence)\n",
    "    return result_sent\n",
    "\n",
    "# Cats -> Cat\n",
    "def lemmatization (sentence):\n",
    "    result_sent = \"\"\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    \n",
    "    sentence = str(sentence)\n",
    "    result_sent = sentence.split()\n",
    "    result_sent = [stemmer.lemmatize(word) for word in sentence]\n",
    "    result_sent = ''.join(result_sent)\n",
    "\n",
    "    return result_sent\n",
    "\n",
    "# amusing, amusement, and amused, the stem would be amuse\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "df_train['title'] = df_train['title'].str.lower()\n",
    "df_train['title'] = df_train['title'].apply(keepAlpha)    \n",
    "df_train['title'] = df_train['title'].apply(removeDigit)   \n",
    "# df_train['title'] = df_train['title'].apply(stemming) # increases or decreases accuracy depending on data\n",
    "# df_train['title'] = df_train['title'].apply(lemmatization) \n",
    "\n",
    "print(df_train['title'])\n",
    "sys.stdout.write('Data cleansing done...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52a76f62fb11264e7af46247aad8dc02a3291eb0"
   },
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc7a837f3106cae89b54ad361383513fbe517147"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(14,8))\n",
    "ax = df_train.groupby('Category_Type').title.count().plot.bar(title=\"Category Distribution\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "05c61e40f6fd61e44c3edc5b6ccad297e3cf6f15"
   },
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c60877531cf95bd4671663282a5dcee0e358e1f9"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# sublinear_df is set to True to use a logarithmic form for frequency.\n",
    "# min_df is the minimum numbers of documents a word must be present in to be kept.\n",
    "# norm is set to l2, to ensure all our feature vectors have a euclidian norm of 1.\n",
    "# ngram_range is set to (1, 2) to indicate that we want to consider both unigrams and bigrams.\n",
    "# stop_words -> remove stopwords\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features = tfidf.fit_transform(df_train['title']).toarray()\n",
    "labels = df_train.Category\n",
    "\n",
    "features.shape\n",
    "sys.stdout.write('Feature Extraction done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ee76996f9f1082d756410fdbabae6bafacb291a7"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "N = 2\n",
    "\n",
    "for Product, category_id in sorted(category_to_id.items()):\n",
    "    \n",
    "    features_chi2 = chi2(features, labels == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "    #print(\"# '{}':\".format(Product))\n",
    "    #print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n",
    "    #print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))\n",
    "\n",
    "sys.stdout.write('Chi2 extraction done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "baddbadac4b8a68a41459cd309bf844e345b0b5b"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['title'], df_train['Category'], random_state = 0)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts) \n",
    "\n",
    "sys.stdout.write('TFIDF done...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a156ed587afafebedb5fab6e5e4d448a71daaec3"
   },
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "874b86ea981cc0b6c09a039c99487a0ceb44887d"
   },
   "outputs": [],
   "source": [
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# clf = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "# print(clf.predict(count_vect.transform([\"new jaminan mut cream baby pink original pemutih kulit wajah obat jerawat perawatan\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3bbec57ea833d21259f8f2b93c65abd427db7819"
   },
   "source": [
    "### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5cad559dece97e57fd65768daf39d3103b9ac2b1"
   },
   "outputs": [],
   "source": [
    "# SVM with Gaussian Kernel\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import samples_generator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),\n",
    "    (\"svm_clf\", SVC(kernel=\"poly\", degree=9, coef0=1, C=5))\n",
    "])\n",
    "\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "sys.stdout.write('Polynomial Kernal done...')\n",
    "# y_pred = clf.predict(count_vect.transform(X_test).astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4f31c695a0f83438a26464fc72342b6976a7d6e2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns;\n",
    "\n",
    "def plot_graph():\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(25,25))\n",
    "    sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "                xticklabels=category_to_id.items(), yticklabels=category_to_id.items())\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()\n",
    "    \n",
    "plot_graph()\n",
    "sys.stdout.write('Plotting of Graph done...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90d2a64953f400c52cc23851039393be8646f4dc"
   },
   "source": [
    "### Trying out different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "98db4e37e1cad6ec4ee1affe655ade36ec15d005"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def diffModel():\n",
    "    models = [\n",
    "        RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "        LinearSVC(),\n",
    "        MultinomialNB(),\n",
    "        LogisticRegression(random_state=0),\n",
    "    ]\n",
    "    CV = 5\n",
    "    cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "    entries = []\n",
    "    for model in models:\n",
    "      model_name = model.__class__.__name__\n",
    "      accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "      for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "    cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "    import seaborn as sns\n",
    "    sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "    sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "                  size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "    plt.show()\n",
    "    \n",
    "# diffModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "039d42594054d825cdcd23d537c486021a843b48"
   },
   "source": [
    "### Use the learning model and export to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1194ee5add981ce2d8ebf64686b8bcec245cd5d7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def checker (df_train):\n",
    "    count = 0\n",
    "    \n",
    "    for itemID, title, category in tqdm(zip(df_train['itemid'], df_train['title'], df_train['Category'])):\n",
    "        ans = clf.predict(count_vect.transform([title]).astype(np.float64))\n",
    "        if (ans == category):\n",
    "            count += 1\n",
    "    \n",
    "    return count\n",
    "    \n",
    "#print (precision_score(count_vect.transform(df_train['Category'], count_vect.transform(X_test).astype(np.float64)))\n",
    "#print(\"{} / {} is correct\".format(checker (df_train), len(df_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "463c71975b1d397fdef478b1d3ef2837ea0eeca7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "# Read from the file\n",
    "df_test = pd.read_csv(\"../input/test.csv\")\n",
    "df_test = df_test[:]\n",
    "\n",
    "def checkTest (df_test):\n",
    "    result = {}\n",
    "    \n",
    "    result['itemid'] = 'Category'\n",
    "    \n",
    "    for itemID, title in tqdm(zip(df_test['itemid'], df_test['title'])):\n",
    "        result[itemID] = clf.predict(count_vect.transform([title]))[0]\n",
    "    \n",
    "    # OrderedDict.fromkeys(result, True)\n",
    "    \n",
    "    return result\n",
    "\n",
    "result = checkTest (df_test)\n",
    "\n",
    "with open('result.csv', 'w') as f:\n",
    "    [f.write('{},{}\\n'.format(key, value)) for key, value in result.items()]\n",
    "sys.stdout.write('Writing to kernel done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b2812618ac174db73fb71280507d097229dd5d29"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
